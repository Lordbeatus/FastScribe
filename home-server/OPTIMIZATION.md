# üöÄ Whisper Performance Optimization Guide

## ‚è±Ô∏è Processing Times Reference

### CPU-Only Performance (Typical Old Computer)

| Video Length | Tiny Model | Base Model | Small Model |
|--------------|------------|------------|-------------|
| 10 minutes   | 30-60 sec  | 1-2 min    | 3-5 min     |
| 30 minutes   | 1.5-3 min  | 3-6 min    | 9-15 min    |
| 1 hour       | 3-6 min    | 6-12 min   | 15-30 min   |
| 2 hours      | 6-12 min   | 12-24 min  | 30-60 min   |

### GPU Performance (NVIDIA GPU)

| Video Length | Tiny Model | Base Model | Small Model |
|--------------|------------|------------|-------------|
| 10 minutes   | 10-20 sec  | 15-30 sec  | 30-60 sec   |
| 30 minutes   | 30-60 sec  | 45-90 sec  | 1.5-3 min   |
| 1 hour       | 1-2 min    | 1.5-3 min  | 3-6 min     |
| 2 hours      | 2-4 min    | 3-6 min    | 6-12 min    |

*Times are approximate and vary based on hardware*

---

## üéØ Optimization Strategies

### 1. Enable GPU Acceleration (10x Faster!)

**Check if you have NVIDIA GPU:**
```bash
# Windows
nvidia-smi

# Should show GPU info if available
```

**Install GPU-enabled PyTorch:**
```bash
pip uninstall torch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Update whisper_server.py:**
```python
# Load model with GPU
model = whisper.load_model("base").cuda()

# In transcribe function, change:
'fp16': True,  # Enable FP16 for GPU (faster!)
```

**Performance gain:** 5-10x faster!

---

### 2. Use Fast Mode for Long Videos

Fast mode trades ~2-3% accuracy for 30-40% speed increase.

**In transcribe request, add:**
```python
data = {'fast': 'true'}
```

**What it does:**
- Greedy decoding (no beam search)
- No temperature fallback
- Faster processing for long audio

**When to use:**
- ‚úÖ Videos over 30 minutes
- ‚úÖ Clear audio quality
- ‚úÖ Speed priority over perfect accuracy

**When NOT to use:**
- ‚ùå Heavy accents
- ‚ùå Poor audio quality
- ‚ùå Need perfect transcription

---

### 3. Choose Right Model for Your Needs

| Model | Size | Speed | Accuracy | Best For |
|-------|------|-------|----------|----------|
| **Tiny** | 39MB | ‚ö°‚ö°‚ö°‚ö°‚ö° | 80-85% | Quick drafts, testing |
| **Base** | 74MB | ‚ö°‚ö°‚ö°‚ö° | 90-95% | ‚≠ê **Recommended** |
| **Small** | 244MB | ‚ö°‚ö°‚ö° | 95-97% | Academic content |
| **Medium** | 769MB | ‚ö°‚ö° | 97-98% | Professional use |
| **Large** | 1550MB | ‚ö° | 98-99% | Maximum accuracy |

**Recommendation for 1-2 hour videos:** **Base model**
- Fast enough (6-12 min per hour on CPU)
- Good accuracy (90-95%)
- Small file size (74MB)

---

### 4. Optimize Audio Preprocessing

**Use yt-dlp to download best quality audio:**
```python
ydl_opts = {
    'format': 'bestaudio[ext=m4a]/bestaudio/best',  # Prefer m4a
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'm4a',  # Faster to process than mp3
    }],
}
```

**Why m4a is faster:**
- Native format for many YouTube videos
- No transcoding needed
- Whisper handles m4a efficiently

---

### 5. Batch Processing (Multiple Videos)

If you need to process many videos, optimize with batching:

**Add to whisper_server.py:**
```python
# Process multiple files at once
@app.route('/transcribe-batch', methods=['POST'])
def transcribe_batch():
    files = request.files.getlist('files[]')
    results = []
    
    for file in files:
        # Process each file
        # (implement similar to single transcribe)
        results.append(result)
    
    return jsonify(results)
```

---

### 6. Hardware Optimizations

**For CPU:**
- Close all other programs during transcription
- Use a cooling pad if laptop (prevents thermal throttling)
- Ensure SSD (not HDD) for temp files

**For GPU:**
- Use latest NVIDIA drivers
- Enable CUDA cores
- Monitor temperature (< 80¬∞C)

**RAM:**
- 4GB minimum for tiny/base
- 8GB recommended for small
- 16GB for medium/large

---

### 7. Network Optimization (Ngrok/Cloudflare)

**For large files (1-2 hour videos ~100-200MB audio):**

**Ngrok optimization:**
```bash
# Increase timeout
ngrok http 8000 --request-timeout 600
```

**Cloudflare Tunnel** (recommended for large files):
- No timeout limits
- Better for 100MB+ files
- More reliable

---

## üìä Real-World Example: 2-Hour Video

### Scenario: 2-hour lecture video

**Option A: CPU + Base Model**
- Download: 30-60 seconds
- Transcribe: 12-24 minutes
- **Total: ~13-25 minutes**

**Option B: GPU + Base Model + Fast Mode**
- Download: 30-60 seconds
- Transcribe: 2-4 minutes
- **Total: ~2.5-5 minutes**

**Option C: CPU + Tiny Model + Fast Mode**
- Download: 30-60 seconds
- Transcribe: 4-8 minutes
- **Total: ~5-9 minutes**
- Accuracy: 80-85% (may have errors)

---

## üí° Recommended Setup for Long Videos

### Best Balance (No GPU)
```python
model = whisper.load_model("base")
transcribe_opts = {
    'language': 'en',
    'fp16': False,
    'beam_size': 1,  # Fast mode
    'condition_on_previous_text': False,
}
```
**Result:** 1-2 hour video in 6-15 minutes, 90%+ accuracy

### Maximum Speed (With GPU)
```python
model = whisper.load_model("tiny").cuda()
transcribe_opts = {
    'language': 'en',
    'fp16': True,
    'beam_size': 1,
}
```
**Result:** 1-2 hour video in 2-5 minutes, 80-85% accuracy

### Maximum Accuracy (With GPU)
```python
model = whisper.load_model("small").cuda()
transcribe_opts = {
    'language': 'en',
    'fp16': True,
}
```
**Result:** 1-2 hour video in 4-8 minutes, 95-97% accuracy

---

## üéØ Quick Decision Tree

**Do you have NVIDIA GPU?**
- ‚úÖ Yes ‚Üí Use `base.cuda()` with `fp16=True` (fast + accurate)
- ‚ùå No ‚Üí Continue below

**How accurate do you need it?**
- üéØ Perfect ‚Üí Use `small` model (~30 min for 2-hour video)
- ‚öñÔ∏è Balanced ‚Üí Use `base` model (~12-24 min for 2-hour video) ‚≠ê
- ‚ö° Fast ‚Üí Use `tiny` model (~6-12 min for 2-hour video)

**Is video over 1 hour?**
- ‚úÖ Yes ‚Üí Enable fast mode (`beam_size=1`)
- ‚ùå No ‚Üí Use default settings

---

## üß™ Testing Your Setup

**Test transcription speed:**
```bash
# Time a 10-minute video
curl -X POST http://localhost:8000/transcribe \
  -F "file=@test_10min.mp3" \
  -F "language=en" \
  -F "fast=true"

# Multiply by 6 for 1-hour estimate
# Multiply by 12 for 2-hour estimate
```

---

## üìà Expected Results

**For typical old computer (no GPU):**
- 1-hour video: **6-12 minutes** (base model)
- 2-hour video: **12-24 minutes** (base model)

**This is acceptable because:**
- ‚úÖ Processing happens in background
- ‚úÖ Still way cheaper than API ($0 vs $8-15/month)
- ‚úÖ You can process overnight
- ‚úÖ Parallel processing possible

**If you need faster, upgrade to GPU setup!**

---

## üöÄ Summary

**Best setup for 1-2 hour videos:**

1. **Model:** Base (74MB) - best balance
2. **Fast Mode:** Enabled for videos > 30 min
3. **GPU:** If available (10x speed boost)
4. **Network:** Cloudflare Tunnel for reliability
5. **Result:** 2-hour video in 6-12 min (CPU) or 2-4 min (GPU)

**Total cost: Still ~$2-5/month! üéâ**
